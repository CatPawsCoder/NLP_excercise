Решающие деревья
Общие понятия
В этой главе рассмотрим новый способ классификации 
и регрессии — решающие деревья (дерево принятия 
решений). Решающие деревья представляют собой направленный иерархический граф. В его узлах стоят признаки, 
по которым идет разделение выборки, а в листьях — части выборки. Причем в левом листе-потомке узла находится та часть 
выборки, для которой признак есть (или для которого ответом 
на вопрос в узле было да), а в правом листе-потомке — часть 
выборки, которая не имеет признака в узле (ответ на вопрос 
в узле — нет). Количество уровней иерархии в решающем дереве называется его глубиной. Например, дерево принятия решений по симптомам болезни (рис. 15).
Как видно из дерево решений имеет глубину 3 и принятие решений начинается с параметра «температура тела». 
Если температура превышает 37 °C, то проверяется скорость 
роста температуры; если меньше, то также идет проверка наличия кашля. Если температура растет быстро, то проверяется 
наличие ломоты в теле; если нет быстрого роста, то предполагается, что диагноз ОРВИ. Если есть ломота в теле, то предполагается диагноз грипп. Если ломоты в теле нет, то проверяется боль в горле, если горло не болит, то рекомендуется вызвать 
скорую помощь (диагноз не ясен). Если болит горло, то предполагается диагноз ангина. Если температура меньше 37 °C
46
Решающие деревья и случайный лес
и есть кашель, то проверяется, есть ли слабость долгое время. 
Если есть слабость долгое время, то рекомендуется обратиться к пульмонологу, если слабости нет, то предполагается диагноз ОРЗ. Также диагноз ОРЗ предполагается, если нет высокой температуры и нет кашля.
Пример решающего дерева
В принципе правила, ведущие к каждому заключению, можно сформировать как логическую формулу. Например, если 
температура меньше 37 °C, есть кашель и слабость долгое время, то нужно обратиться к пульмонологу.
Рассмотрим, как применяется решающее дерево к выборке. Допустим, у нас есть 50 пациентов, по которым заполнены 
данные о температуре тела, скорости ее роста, наличие кашля, 
ломоты и боли в горле. Тогда на первом шаге применения решающего дерева (рис. 15) выборка разделится на 2 части: в одной будут пациенты с температурой выше 37 °C, в другой — 
ниже 37 °C. Далее часть выборки с пациентами, имеющими 
температуру выше 37 °C, будет снова разделена на две части. 
В одной — будут пациенты с быстро растущей температурой, 
Решающие деревья
в другой — со стабильной температурой. Затем часть выборки с быстро растущей температурой снова будет разделена 
по признаку наличия ломоты в теле. Та часть выборки, для которой признак ломоты в теле отсутствует, будет снова поделена 
по признаку боли в горле. Аналогично будет разделена и часть 
выборки с температурой тела ниже 37 °C. В конечном итоге будут получены 7 групп (по числу листов дерева) из первоначальной выборки.
Решающие деревья чаще всего применяются для задач классификации. Как известно, геометрический смысл классификации сводится к нахождению линии или плоскости, разделяющей объекты выборки на классы.  приведен пример 
классификации с помощью решающего дерева.
выборка поделена на 3 класса — синий, 
желтый и красный. Для этого потребовалось дерево с 2 уровнями иерархии. На первом (самом верхнем уровне) провели 
Решающие деревья и случайный лес
разделение на синий и желтый классы, на втором уровне желтый класс разделили на красный и желтый классы. Некоторые 
объекты желтого класса попали в красный класс (см. рис. 16). 
Что же будет, если продолжить разделение выборки по классам, 
применяя новые признаки? Скорее всего, модель переобучится. Решающие деревья очень легко переобучаются, то есть решающее дерево можно очень хорошо «заточить» под конкретную выборку, но потом его нельзя будет применять для другой 
выборки. Обучение решающих деревьев
Построение решающих деревьев идет путем разделения выборки на части по вводимым признакам. Признаки и порог их 
значения, по которым делится выборка, нужно подбирать так, 
чтобы в листьях дерева оставались объекты одного класса.
Пусть в вершине Xm объектов. Выбираем порог t по критерию ошибки Q для признака j, минимизируя критерий ошибки:
Q X j t m ( , , )® min,
причем признаки j и значения порога перебираем так, чтобы выполнить данное условие. В итоге получаем две части выборки:
X x X x t l m
j = О й Ј л щ { }ы ,
X x X x t r m
j = О й > л щ { }ы .
Однако дерево нельзя строить до бесконечности: нужно 
знать, когда остановиться. Остановка в разбиении происходит в 3 случаях. В первом случае, если в вершину попал только 
один объект обучающей выборки или все объекты принадлежат 
одному классу (в задачах классификации). Во втором случае, 
если глубина дерева достигла определенного значения. В третьем случае задают количество объектов в листе дерева. Причем чаще на практике задают глубину разделения.